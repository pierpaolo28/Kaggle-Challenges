{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Credit Card Fraud - Model Tuning"},{"metadata":{},"cell_type":"markdown","source":"Machine Learning models tuning is a type of optimization problem. We have a set of hyperparameters and we aim to find the right combination of their values which can help us to find either the minimum (eg. loss) or the maximum (eg. accuracy) of a function.\n\nBelow, is shown a really simple example of how to find the minimum of a function using Scipy. \n\nIn this Notebook, I will walk you through different techniques commonly used in Machine Learning to optimise Hyperparameters selection to improve accuracy results.\n\n**Table of Contents:**\n1. Manual Search\n2. Random Search\n3. Grid Search\n4. Automated Hyperparameter Tuning (Bayesian Optimization, Genetic Algorithms)\n5. Artificial Neural Networks (ANNs) Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://scipy-lectures.org/intro/scipy/auto_examples/plot_optimize_example2.html\nimport numpy as np \nimport matplotlib.pyplot as plt\nfrom scipy import optimize\n\n# Creating a function to examine\nx = np.arange(-20, 15, 0.3)\ndef f(x):\n    return x**2 - (5*x)/7 - 50*np.cos(x)\n\n# Global optimization\ngrid = (-20, 15, 0.3)\nxmin_global = optimize.brute(f, (grid, ))\nprint(\"Global minima (-20-15) at: {}\".format(float(xmin_global)))\n\n# Constrained optimization\nxmin_local = optimize.fminbound(f, 5, 15)\nprint(\"Local minimum (5-15) at: {}\".format(xmin_local))\n\n# Plotting the function\nfig = plt.figure(figsize=(10, 8))\nplt.plot(x, f(x), 'b', label=\"f(x)\")\n\n# Plotting horizontal line where possible roots can be found \nplt.axhline(0, color='gray', label=\"Roots Level\")\n\n# Plotting the function minima\nxmins = np.array([xmin_global[0], xmin_local])\nplt.plot(xmins, f(xmins), 'go', label=\"Minima\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.title(\"Finding the minimum of a function\")\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A machine learning model can be composed by two different types of parameters:\n- Hyperparamters are all the parameters which can be arbitrarely set by the user before starting training (eg. number of estimators in Random Forest). \n- Model parameters are instead learned during the model training (eg. weights in Neural Networks, Linear Regression).\n\nThe model parameters define how to use input data to get a desired output, while the hyperparameters determine how our model is structured in the first place. \n\nThis can be particularly important when comparing how different Machine Learning models performs on a dataset. In fact, I would be unfair for example to compare an SVM model with the best hyperparameters against a Random Forest model with has not been optimized.\n\nI will now walk you through a practical example using Kaggle Credit Card Fraud Dataset. In this case, I decided to use just a subset of the dataset, in order to speed up training times and make sure to achieve a perfect balance between the two different classes. Additionally, just a limited amount of features will be used in order to make the optimization tasks more challenging."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib.pyplot import figure\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/creditcardfraud/creditcard.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.shape)\nprint(df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"percent_missing = df.isnull().sum() * 100 / len(df)\nmissing_values = pd.DataFrame({'percent_missing': percent_missing})\nmissing_values.sort_values(by ='percent_missing' , ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure(num=None, figsize=(10, 8), dpi=80, facecolor='w', edgecolor='k')\n\nsns.set(style=\"ticks\")\nf = sns.countplot(x=\"Class\", data=df, palette=\"bwr\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure(num=None, figsize=(10, 8), dpi=80, facecolor='w', edgecolor='k')\n\ncorr=df.corr()\nsns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df[df.Class == 1][0:400]\nprint(df2.shape)\ndf3 = df[df.Class == 0][0:400]\nprint(df3.shape)\n\ndf = df2.append(df3, ignore_index=True)\n#df4.head()\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure(num=None, figsize=(10, 8), dpi=80, facecolor='w', edgecolor='k')\n\nsns.set(style=\"ticks\")\nf = sns.countplot(x=\"Class\", data=df, palette=\"bwr\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['Class'], axis = 1).values\nY = df['Class']\n\nX = StandardScaler().fit_transform(X)\n\nX_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.30, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=300).fit(X_Train,Y_Train)\npredictionforest = model.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure(num=None, figsize=(10, 8), dpi=80, facecolor='w', edgecolor='k')\n\nfeat_importances = pd.Series(model.feature_importances_, index=df.drop(df[['Class']], \n                                                                       axis=1).columns)\nfeat_importances.nlargest(30).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[['V17', 'V9', 'V6', 'V12']]\nY = df['Class']\n\nX = StandardScaler().fit_transform(X)\n\nX_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.30, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(random_state= 101).fit(X_Train,Y_Train)\npredictionforest = model.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))\nacc1 = accuracy_score(Y_Test,predictionforest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Manual Search"},{"metadata":{},"cell_type":"markdown","source":"When using Manual Search, we choose some model hyperparameters based on our judgment/experience. We then train the model, evaluate its accuracy and start the process again. This loop is repeated until a satisficatory accuracy is scored."},{"metadata":{},"cell_type":"markdown","source":"The main parameters used by a Random Forest Classifier are:\n- criterion = the function used to evaluate the quality of a split.\n- max_depth = maximum number of levels allowed in each tree.\n- max_features = maximum number of features considered when splitting a node.\n- min_samples_leaf = minimum number of samples which can be stored in a tree leaf.\n- min_samples_split = minimum number of samples necessary in a node to cause node splitting.\n- n_estimators = number of trees in the ensamble."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=10, random_state= 101).fit(X_Train,Y_Train)\npredictionforest = model.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))\nacc2 = accuracy_score(Y_Test,predictionforest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators= 200, max_features = \"log2\", min_samples_leaf = 20,\n                               random_state= 101).fit(X_Train,Y_Train)\npredictionforest = model.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Search"},{"metadata":{},"cell_type":"markdown","source":"In Random Search, we create a grid of hyperparameters and train/test our model on just some random combination of these hyperparameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\nrandom_search = {'criterion': ['entropy', 'gini'],\n               'max_depth': list(np.linspace(10, 1200, 10, dtype = int)) + [None],\n               'max_features': ['auto', 'sqrt','log2', None],\n               'min_samples_leaf': [4, 12],\n               'min_samples_split': [5, 10],\n               'n_estimators': list(np.linspace(151, 1200, 10, dtype = int))}\n\nprint(random_search)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this example, I additionally decided to perform Cross-Validation on the training set. When performing Machine Learning tasks, we generally divide our dataset in training and test sets. This is done so that to test our model after having trained it to check it's performances when working with unseen data. When using Cross-Validation, we divide our training set in N other partitions to make sure our model is not overfitting our data. \n\nOne of the most common used Cross-Validation methods is K-Fold Validation. In K-Fold, we divide our dataset in N partitions and then iteratively train our model using N-1 partitions and test it with the left-over partition (at each iteration we change the left-over partition). Once having trained N times our model we then average the training results obtained in each iteration to obtain our overall training performance results.\n\n![](http://ethen8181.github.io/machine-learning/model_selection/img/kfolds.png)"},{"metadata":{},"cell_type":"markdown","source":"Using Cross-Validation when implementing Hyperparameters optimization can be really important. In this way, we might avoid using some Hyperparameters which works really good on the training data but not so good with the test data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier()\nmodel = RandomizedSearchCV(estimator = clf, param_distributions = random_search, n_iter = 80, \n                               cv = 4, verbose= 5, random_state= 101, n_jobs = -1)\nmodel.fit(X_Train,Y_Train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictionforest = model.best_estimator_.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))\nacc3 = accuracy_score(Y_Test,predictionforest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid Search"},{"metadata":{},"cell_type":"markdown","source":"In Grid Search, we set up a grid of hyperparameters and train/test our model on each of the possible combinations. \n"},{"metadata":{},"cell_type":"markdown","source":"In order to choose the parameters to use in Grid Search, we can try to run first Random Search to get an understanding of which values might perform best on the considered dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ngrid_search = {\n    'criterion': [model.best_params_['criterion']],\n    'max_depth': [model.best_params_['max_depth']],\n    'max_features': [model.best_params_['max_features']],\n    'min_samples_leaf': [model.best_params_['min_samples_leaf'] - 2, \n                         model.best_params_['min_samples_leaf'], \n                         model.best_params_['min_samples_leaf'] + 2],\n    'min_samples_split': [model.best_params_['min_samples_split'] - 3, \n                          model.best_params_['min_samples_split'], \n                          model.best_params_['min_samples_split'] + 3],\n    'n_estimators': [model.best_params_['n_estimators'] - 150, model.best_params_['n_estimators'] - 100, \n                     model.best_params_['n_estimators'], \n                     model.best_params_['n_estimators'] + 100, model.best_params_['n_estimators'] + 150]\n}\n\nprint(grid_search)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier()\nmodel = GridSearchCV(estimator = clf, param_grid = grid_search, \n                               cv = 4, verbose= 5, n_jobs = -1)\nmodel.fit(X_Train,Y_Train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictionforest = model.best_estimator_.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))\nacc4 = accuracy_score(Y_Test,predictionforest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Grid Search is slower compared to Random Search but can be more effective because it can go through the whole search space. Instead, Random Search can be faster fast, but might miss some important points in the search space."},{"metadata":{},"cell_type":"markdown","source":"## Automated Hyperparameter Tuning"},{"metadata":{},"cell_type":"markdown","source":"Using Automated Hyperparameter Tuning, the model hyperparameters to try are identified using techniques such as: Bayesian Optimization, Gradient Descent and Evolutionary Algorithms. "},{"metadata":{},"cell_type":"markdown","source":"### Bayesian Optimization"},{"metadata":{},"cell_type":"markdown","source":"Hyperopt is a Python library used to optimize Machine Learning models using Bayesian Optimization. Bayesian optimization uses probability to find the minimum of a function. The final aim is to find the input value to a function which can gives us the lowest possible output value. <br>\n\nBaysian optimization has been proved to be more efficient than random, grid or manual search. Bayesian Optimization can therefore lead to better performance in the testing phase and reduced optimization time."},{"metadata":{},"cell_type":"markdown","source":"In Hyperopt, Bayesian Optimization can be implemented giving 3 three main parameters to the function fmin. <br>\n\n- Objective Function = defines the loss function to minimize.\n- Domain Space = defines the range of input values to test (in Bayesian Optimization this space creates a probability distribution for each of the used Hyperparameters).\n- Optimization Algorithm = defines the search algorithm to use to select the best input values to use in each new iteration. \n\nAdditionally, can also be defined in fmin the maximum number of evaluations to perform."},{"metadata":{},"cell_type":"markdown","source":"Bayesian Optimization can reduce the number of search iterations by choosing the input values bearing in mind the past outcomes. In this way, we can concentrate our search from the beginning on values which are closer to our desired output."},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n        'max_depth': hp.quniform('max_depth', 10, 1200, 10),\n        'max_features': hp.choice('max_features', ['auto', 'sqrt','log2', None]),\n        'min_samples_leaf': hp.uniform ('min_samples_leaf', 0, 0.5),\n        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n        'n_estimators' : hp.choice('n_estimators', [10, 50, 300, 750, 1200])\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(space):\n    model = RandomForestClassifier(criterion = space['criterion'], max_depth = space['max_depth'],\n                                 max_features = space['max_features'],\n                                 min_samples_leaf = space['min_samples_leaf'],\n                                 min_samples_split = space['min_samples_split'],\n                                 n_estimators = space['n_estimators'], \n                                 )\n    \n    accuracy = cross_val_score(model, X_Train, Y_Train, cv = 4).mean()\n\n    # We aim to maximize accuracy, therefore we return it as a negative value\n    return {'loss': -accuracy, 'status': STATUS_OK }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now run our Bayesian Optimizer using the fmin() function. A Trials() object is first created to make possible later to visualize later what was going on while the fmin() function was running (eg. how the loss function was changing and how to used Hyperparameters were changing)."},{"metadata":{"trusted":true},"cell_type":"code","source":"trials = Trials()\nbest = fmin(fn= objective,\n            space= space,\n            algo= tpe.suggest,\n            max_evals = 80,\n            trials= trials)\nbest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From: https://medium.com/district-data-labs/parameter-tuning-with-hyperopt-faa86acdfdce\nparameters = ['criterion', 'max_depth', 'max_features', 'min_samples_leaf', 'min_samples_split',\n              'n_estimators']\nf, axes = plt.subplots(nrows=3, ncols=2, figsize=(15,10))\nf.tight_layout()\ncmap = plt.cm.jet\nfor i, val in enumerate(parameters):\n    print(i, val)\n    xs = np.array([t['misc']['vals'][val] for t in trials.trials]).ravel()\n    ys = [-t['result']['loss'] for t in trials.trials]\n    xs, ys = zip(*sorted(zip(xs, ys)))\n    ys = np.array(ys)\n    axes[i//2,i%2].scatter(xs, ys, s=20, linewidth=0.01, alpha=0.5, c=cmap(float(i)/len(parameters)))\n    axes[i//2,i%2].set_title(val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crit = {0: 'entropy', 1: 'gini'}\nfeat = {0: 'auto', 1: 'sqrt', 2: 'log2', 3: None}\nest = {0: 10, 1: 50, 2: 300, 3: 750, 4: 1200}\n\nprint(crit[best['criterion']])\nprint(feat[best['max_features']])\nprint(est[best['n_estimators']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainedforest = RandomForestClassifier(criterion = crit[best['criterion']], max_depth = best['max_depth'], \n                                       max_features = feat[best['max_features']], \n                                       min_samples_leaf = best['min_samples_leaf'], \n                                       min_samples_split = best['min_samples_split'], \n                                       n_estimators = est[best['n_estimators']]).fit(X_Train,Y_Train)\npredictionforest = trainedforest.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))\nacc5 = accuracy_score(Y_Test,predictionforest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Genetic Algorithms"},{"metadata":{},"cell_type":"markdown","source":"Genetic Algorithms tries to apply natural selection mechanisms to Machine Learning contexts. \n\nLet's immagine we create a population of N Machine Learning models with some predifined Hyperparameters. We can then calculate the accuracy of each model and decide to keep just half of the models (the ones that performs best). We can now generate some offsprings having similar Hyperparameters to the ones of the best models so that go get again a population of N models. At this point we can again caltulate the accuracy of each model and repeate the cycle for a defined number of generations. In this way, just the best models will survive at the end of the process."},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'criterion': ['entropy', 'gini'],\n               'max_depth': list(np.linspace(10, 1200, 10, dtype = int)) + [None],\n               'max_features': ['auto', 'sqrt','log2', None],\n               'min_samples_leaf': [4, 12],\n               'min_samples_split': [5, 10],\n               'n_estimators': list(np.linspace(151, 1200, 10, dtype = int))}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tpot import TPOTClassifier\nfrom deap.gp import Primitive\n\n\ntpot_classifier = TPOTClassifier(generations= 5, population_size= 24, offspring_size= 12,\n                                 verbosity= 2, early_stop= 12,\n                                 config_dict={'sklearn.ensemble.RandomForestClassifier': parameters}, \n                                 cv = 4, scoring = 'accuracy')\ntpot_classifier.fit(X_Train,Y_Train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tpot_classifier.score(X_Test, Y_Test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://medium.com/cindicator/genetic-algorithms-and-hyperparameters-weekend-of-a-data-scientist-8f069669015e\nargs = {}\nfor arg in tpot_classifier._optimized_pipeline:\n    if type(arg) != Primitive:\n        try:\n            if arg.value.split('__')[1].split('=')[0] in ['criterion', 'max_depth', \n                                                          'max_features', 'min_samples_leaf', \n                                                          'min_samples_split',\n                                                          'n_estimators']:\n                args[arg.value.split('__')[1].split('=')[0]] = int(arg.value.split('__')[1].split('=')[1])\n            else:\n                args[arg.value.split('__')[1].split('=')[0]] = float(arg.value.split('__')[1].split('=')[1])\n        except:\n            pass\nparams = args","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier( max_depth = params['max_depth'],\n                             min_samples_leaf = params['min_samples_leaf'],\n                             min_samples_split = params['min_samples_split'],\n                             n_estimators = params['n_estimators'], \n                             )\nmodel.fit(X_Train,Y_Train)\npredictionforest = model.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))\nacc6 = accuracy_score(Y_Test,predictionforest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Artificial Neural Networks (ANNs) Tuning"},{"metadata":{},"cell_type":"markdown","source":"Using KerasClassifier wrapper, it is possible to apply Grid Search and Random Search for Deep Learning models in the same way it was done when using Sklearn Machine Learning models. More examples are available here:\n- https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\ndef DL_Model(activation= 'linear', neurons= 5, optimizer='Adam'):\n    model = Sequential()\n    model.add(Dense(neurons, input_dim= 4, activation= activation))\n    model.add(Dense(neurons, activation= activation))\n    model.add(Dropout(0.3))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer= optimizer, metrics=['accuracy'])\n    return model\n\n# Definying grid parameters\nactivation = ['softmax', 'relu', 'tanh', 'sigmoid', 'linear']\nneurons = [5, 10, 15, 25, 35, 50]\noptimizer = ['SGD', 'Adam', 'Adamax']\nparam_grid = dict(activation = activation, neurons = neurons, optimizer = optimizer)\n\nclf = KerasClassifier(build_fn= DL_Model, epochs= 80, batch_size=40, verbose= 0)\n\nmodel = GridSearchCV(estimator= clf, param_grid=param_grid, n_jobs=-1)\nmodel.fit(X_Train,Y_Train)\n\nprint(\"Max Accuracy Registred: {} using {}\".format(round(model.best_score_,3), model.best_params_))\nacc = model.cv_results_['mean_test_score']\nhyper = model.cv_results_['params']\n\nfor mean, param in zip(acc, hyper):\n    print(\"Overall accuracy of {} % using: {}\".format(round(mean, 3), param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictionforest = model.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))\nacc7 = accuracy_score(Y_Test,predictionforest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Comparison"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Base Accuracy vs Manual Search {:0.4f}%.'.format( 100 * (acc2 - acc1) / acc1))\nprint('Base Accuracy vs Random Search {:0.4f}%.'.format( 100 * (acc3 - acc1) / acc1))\nprint('Base Accuracy vs Grid Search {:0.4f}%.'.format( 100 * (acc4 - acc1) / acc1))\nprint('Bayesian Optimization Accuracy vs Manual Search {:0.4f}%.'.format( 100 * (acc5 - acc1) / acc1))\nprint('Evolutionary Algorithms vs Manual Search {:0.4f}%.'.format( 100 * (acc6 - acc1) / acc1))\nprint('Optimized ANN vs Manual Search {:0.4f}%.'.format( 100 * (acc7 - acc1) / acc1))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}